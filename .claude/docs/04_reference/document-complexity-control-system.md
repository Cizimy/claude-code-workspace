# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¤‡é›‘æ€§åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ  æŠ€è¡“ä»•æ§˜

> **ç›®çš„**: ADR-005ã§æ±ºå®šã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¤‡é›‘æ€§åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã®æŠ€è¡“è©³ç´°ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ»å®Ÿè£…ä»•æ§˜

## ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¢ãƒ‡ãƒ«
```
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: Markdown + SQLite/DuckDB ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é›†ç´„
```

```mermaid
graph TD
    A[Markdown Files] --> B[Front-Matter Extraction]
    B --> C[JSON Schema Validation]
    C --> D[SQLite Database]
    D --> E[æ¨ªæ–­æ¤œç´¢ãƒ»åˆ†æ]
    D --> F[é™çš„ã‚µã‚¤ãƒˆç”Ÿæˆ]
    D --> G[KPI ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰]
    
    H[CI/CD Pipeline] --> C
    I[AIå®Œç’§ä¸»ç¾©é˜²æ­¢] --> A
    J[ç¶™ç¶šæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ] --> E
```

### è¨­è¨ˆåŸå‰‡
1. **Markdownä¸€æ¬¡ã‚½ãƒ¼ã‚¹ç¶­æŒ**: Gitå±¥æ­´ãƒ»æ¨©é™ãƒ»diffè¿½è·¡ã®ä¿æŒ
2. **æ§‹é€ åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼è¿½åŠ **: SQLite ã«ã‚ˆã‚‹æ¨ªæ–­æ¤œç´¢ãƒ»é›†è¨ˆæ©Ÿèƒ½
3. **æ®µéšçš„å°å…¥**: Week 1-6ã®è¨ˆç”»çš„å®Ÿè£…ã«ã‚ˆã‚‹æœ€å°ãƒªã‚¹ã‚¯
4. **æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ**: AIå®Œç’§ä¸»ç¾©é˜²æ­¢ãƒ»ç¶™ç¶šæ”¹å–„ã¨ã®ç›¸ä¹—åŠ¹æœ

---

## ğŸ—ï¸ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°ä»•æ§˜

### 1. Front-Matteræ¨™æº–åŒ–

#### 1.1 å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©
```yaml
---
title: "æ–‡æ›¸ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå¿…é ˆï¼‰"
status: "draft|active|deprecated"  # å¿…é ˆ
category: "governance|operations|reference|templates|quickstart"  # å¿…é ˆ
created: "YYYY-MM-DD"  # å¿…é ˆ
updated: "YYYY-MM-DD"  # å¿…é ˆ
---
```

#### 1.2 ä»»æ„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©
```yaml
---
# é–¢é€£æ€§ãƒ»åˆ†é¡
adr: "005"  # é–¢é€£ADRç•ªå·
tags: ["complexity", "governance"]  # è¤‡æ•°ã‚¿ã‚°
priority: "high|medium|low"

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
author: "ã‚·ã‚¹ãƒ†ãƒ å/æ‹…å½“è€…"
version: "1.0"
review_date: "YYYY-MM-DD"

# é–¢ä¿‚æ€§
depends_on: ["001-base-adr.md"]  # ä¾å­˜æ–‡æ›¸
supersedes: ["old-doc.md"]      # ç½®æ›å¯¾è±¡
---
```

#### 1.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ä»•æ§˜

##### governance
```yaml
---
title: "ADR-NNN: æ±ºå®šäº‹é …ã‚¿ã‚¤ãƒˆãƒ«"
status: "proposed|accepted|rejected|superseded"
category: "governance"
adr: "NNN"
decision_date: "YYYY-MM-DD"
review_date: "YYYY-MM-DD"
stakeholders: ["role1", "role2"]
---
```

##### operations
```yaml
---
title: "é‹ç”¨æ‰‹é †æ›¸ã‚¿ã‚¤ãƒˆãƒ«"
status: "active"
category: "operations"
procedure_type: "daily|weekly|monthly|emergency"
automation_level: "manual|semi|auto"
dependencies: ["tool1", "tool2"]
---
```

##### reference
```yaml
---
title: "æŠ€è¡“ä»•æ§˜æ›¸ã‚¿ã‚¤ãƒˆãƒ«"
status: "active"
category: "reference"
technical_domain: "hooks|commands|architecture"
complexity_level: "basic|intermediate|advanced"
implementation_status: "design|prototype|stable"
---
```

### 2. JSON Schemaä»•æ§˜

#### 2.1 åŸºæœ¬ã‚¹ã‚­ãƒ¼ãƒæ§‹é€ 
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Claude Code Workspace Document Metadata",
  "type": "object",
  "required": ["title", "status", "category", "created", "updated"],
  "properties": {
    "title": {
      "type": "string",
      "minLength": 5,
      "maxLength": 100,
      "description": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«"
    },
    "status": {
      "type": "string",
      "enum": ["draft", "active", "deprecated", "proposed", "accepted", "rejected", "superseded"],
      "description": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"
    },
    "category": {
      "type": "string", 
      "enum": ["governance", "operations", "reference", "templates", "quickstart"],
      "description": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚«ãƒ†ã‚´ãƒª"
    },
    "created": {
      "type": "string",
      "format": "date",
      "description": "ä½œæˆæ—¥"
    },
    "updated": {
      "type": "string", 
      "format": "date",
      "description": "æœ€çµ‚æ›´æ–°æ—¥"
    },
    "adr": {
      "type": "string",
      "pattern": "^[0-9]{3}$",
      "description": "é–¢é€£ADRç•ªå·ï¼ˆ3æ¡ï¼‰"
    },
    "tags": {
      "type": "array",
      "items": {"type": "string"},
      "maxItems": 10,
      "description": "åˆ†é¡ã‚¿ã‚°"
    },
    "priority": {
      "type": "string",
      "enum": ["high", "medium", "low"],
      "description": "å„ªå…ˆåº¦"
    }
  },
  "additionalProperties": false
}
```

#### 2.2 ã‚«ãƒ†ã‚´ãƒªåˆ¥æ‹¡å¼µã‚¹ã‚­ãƒ¼ãƒ

##### governance.schema.json
```json
{
  "allOf": [
    {"$ref": "base.schema.json"},
    {
      "properties": {
        "category": {"const": "governance"},
        "decision_date": {"type": "string", "format": "date"},
        "review_date": {"type": "string", "format": "date"},
        "stakeholders": {
          "type": "array",
          "items": {"type": "string"}
        }
      },
      "required": ["decision_date"]
    }
  ]
}
```

### 3. SQLiteçµ±åˆä»•æ§˜

#### 3.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ
```sql
-- ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE docs_index (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT UNIQUE NOT NULL,
    file_name TEXT NOT NULL,
    title TEXT NOT NULL,
    status TEXT NOT NULL,
    category TEXT NOT NULL,
    created DATE NOT NULL,
    updated DATE NOT NULL,
    
    -- ä»»æ„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    adr TEXT,
    priority TEXT,
    author TEXT,
    version TEXT,
    review_date DATE,
    
    -- è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    lines INTEGER,
    internal_links INTEGER,
    external_links INTEGER,
    words INTEGER,
    
    -- ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    checksum TEXT,  -- å¤‰æ›´æ¤œå‡ºç”¨
    
    -- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    CONSTRAINT status_check CHECK (status IN ('draft', 'active', 'deprecated', 'proposed', 'accepted', 'rejected', 'superseded')),
    CONSTRAINT category_check CHECK (category IN ('governance', 'operations', 'reference', 'templates', 'quickstart'))
);

-- ã‚¿ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ­£è¦åŒ–ï¼‰
CREATE TABLE doc_tags (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    doc_id INTEGER REFERENCES docs_index(id) ON DELETE CASCADE,
    tag TEXT NOT NULL,
    UNIQUE(doc_id, tag)
);

-- é–¢ä¿‚æ€§ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE doc_relationships (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_doc_id INTEGER REFERENCES docs_index(id) ON DELETE CASCADE,
    target_doc_id INTEGER REFERENCES docs_index(id) ON DELETE CASCADE,
    relationship_type TEXT NOT NULL, -- depends_on, supersedes, references
    UNIQUE(source_doc_id, target_doc_id, relationship_type)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_docs_status ON docs_index(status);
CREATE INDEX idx_docs_category ON docs_index(category);
CREATE INDEX idx_docs_updated ON docs_index(updated);
CREATE INDEX idx_tags_tag ON doc_tags(tag);
```

#### 3.2 ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

##### build_docs_db.py
```python
#!/usr/bin/env python3
"""
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»SQLiteçµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import re
import sqlite3
import hashlib
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

class DocumentProcessor:
    def __init__(self, db_path: str = "build/docs_index.sqlite"):
        self.db_path = db_path
        self.conn = None
        
    def connect(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ»åˆæœŸåŒ–"""
        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute("PRAGMA foreign_keys = ON")
        self._create_tables()
        
    def _create_tables(self):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        with open('schemas/sqlite_schema.sql', 'r') as f:
            schema = f.read()
        self.conn.executescript(schema)
        
    def extract_frontmatter(self, content: str) -> Optional[Dict]:
        """Front-MatteræŠ½å‡º"""
        if not content.startswith('---\n'):
            return None
            
        try:
            _, yaml_content, _ = content.split('---\n', 2)
            return yaml.safe_load(yaml_content)
        except (ValueError, yaml.YAMLError) as e:
            print(f"Front-Matter parse error: {e}")
            return None
            
    def analyze_content(self, content: str) -> Dict:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ"""
        return {
            'lines': len(content.splitlines()),
            'words': len(content.split()),
            'internal_links': len(re.findall(r'\[.*?\]\([^)]*\.md\)', content)),
            'external_links': len(re.findall(r'\[.*?\]\(https?://[^)]+\)', content))
        }
        
    def process_file(self, file_path: Path) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
            checksum = hashlib.md5(content.encode()).hexdigest()
            
            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèª
            existing = self.conn.execute(
                "SELECT checksum FROM docs_index WHERE file_path = ?", 
                (str(file_path),)
            ).fetchone()
            
            if existing and existing[0] == checksum:
                return False  # å¤‰æ›´ãªã—
                
            # Front-MatteræŠ½å‡º
            frontmatter = self.extract_frontmatter(content)
            if not frontmatter:
                print(f"âš ï¸ No front-matter: {file_path}")
                return False
                
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ
            analysis = self.analyze_content(content)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            self._upsert_document(file_path, frontmatter, analysis, checksum)
            return True
            
        except Exception as e:
            print(f"âŒ Error processing {file_path}: {e}")
            return False
            
    def _upsert_document(self, file_path: Path, frontmatter: Dict, analysis: Dict, checksum: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ã‚³ãƒ¼ãƒ‰æ›´æ–°ãƒ»æŒ¿å…¥"""
        # ãƒ¡ã‚¤ãƒ³ãƒ¬ã‚³ãƒ¼ãƒ‰
        self.conn.execute("""
            INSERT OR REPLACE INTO docs_index (
                file_path, file_name, title, status, category, created, updated,
                adr, priority, author, version, review_date,
                lines, internal_links, external_links, words,
                checksum, processed_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            str(file_path), file_path.name,
            frontmatter.get('title', ''), frontmatter.get('status', ''),
            frontmatter.get('category', ''), frontmatter.get('created', ''),
            frontmatter.get('updated', ''), frontmatter.get('adr'),
            frontmatter.get('priority'), frontmatter.get('author'),
            frontmatter.get('version'), frontmatter.get('review_date'),
            analysis['lines'], analysis['internal_links'],
            analysis['external_links'], analysis['words'],
            checksum, datetime.now()
        ))
        
        doc_id = self.conn.lastrowid
        
        # ã‚¿ã‚°æ›´æ–°
        self.conn.execute("DELETE FROM doc_tags WHERE doc_id = ?", (doc_id,))
        for tag in frontmatter.get('tags', []):
            self.conn.execute(
                "INSERT INTO doc_tags (doc_id, tag) VALUES (?, ?)",
                (doc_id, tag)
            )
            
        self.conn.commit()
        
    def process_all(self) -> Dict:
        """å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†"""
        stats = {'processed': 0, 'updated': 0, 'errors': 0}
        
        for md_file in Path('.').rglob('*.md'):
            if self._should_skip(md_file):
                continue
                
            stats['processed'] += 1
            if self.process_file(md_file):
                stats['updated'] += 1
                
        return stats
        
    def _should_skip(self, file_path: Path) -> bool:
        """å‡¦ç†é™¤å¤–åˆ¤å®š"""
        skip_patterns = [
            'node_modules/', '.git/', 'build/', '__pycache__/',
            'README.md'  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆREADMEã¯é™¤å¤–
        ]
        return any(pattern in str(file_path) for pattern in skip_patterns)

if __name__ == "__main__":
    processor = DocumentProcessor()
    processor.connect()
    stats = processor.process_all()
    print(f"ğŸ“Š å‡¦ç†å®Œäº†: {stats}")
```

### 4. CIçµ±åˆä»•æ§˜

#### 4.1 GitHub Actionsè¨­å®š
```yaml
# .github/workflows/docs-ci.yml
name: Documentation Quality & Complexity Control

on:
  push:
    paths: ['**/*.md']
  pull_request:
    paths: ['**/*.md']

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.9'

jobs:
  docs-validation:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install Dependencies
        run: |
          npm install -g remark-cli remark-lint-frontmatter-schema
          pip install pyyaml jsonschema
          
      - name: Front-Matter Schema Validation
        run: |
          echo "ğŸ” Front-Matter validation..."
          find . -name "*.md" -not -path "./node_modules/*" -not -path "./.git/*" | \
          xargs python scripts/validate_frontmatter.py
          
      - name: Document Complexity Check
        run: |
          echo "ğŸ“Š Complexity analysis..."
          python scripts/doc_inventory.py --ci-mode
          
      - name: Internal Link Validation
        run: |
          echo "ğŸ”— Link validation..."
          python scripts/link_checker.py --strict
          
      - name: SQLite Database Update
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "ğŸ’¾ Database update..."
          python scripts/build_docs_db.py
          
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: docs-analysis
          path: |
            build/docs_analysis.json
            build/docs_index.sqlite
```

#### 4.2 æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆä»•æ§˜

##### validate_frontmatter.py
```python
#!/usr/bin/env python3
import sys
import json
import yaml
import jsonschema
from pathlib import Path

def validate_document(file_path: Path, schema: dict) -> bool:
    """å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®Front-Matteræ¤œè¨¼"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        if not content.startswith('---\n'):
            print(f"âŒ {file_path}: Front-Matter missing")
            return False
            
        _, yaml_content, _ = content.split('---\n', 2)
        frontmatter = yaml.safe_load(yaml_content)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚­ãƒ¼ãƒé¸æŠ
        category = frontmatter.get('category', 'base')
        schema_file = f"schemas/{category}.schema.json"
        
        if Path(schema_file).exists():
            with open(schema_file, 'r') as f:
                category_schema = json.load(f)
            jsonschema.validate(frontmatter, category_schema)
        else:
            jsonschema.validate(frontmatter, schema)
            
        print(f"âœ… {file_path}: Valid")
        return True
        
    except Exception as e:
        print(f"âŒ {file_path}: {e}")
        return False

def main():
    # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒèª­ã¿è¾¼ã¿
    with open('schemas/md-meta.schema.json', 'r') as f:
        base_schema = json.load(f)
        
    # å…¨Markdownãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
    errors = 0
    for md_file in Path('.').rglob('*.md'):
        if 'node_modules' in str(md_file) or '.git' in str(md_file):
            continue
            
        if not validate_document(md_file, base_schema):
            errors += 1
            
    if errors > 0:
        print(f"ğŸ’¥ {errors} validation errors found")
        sys.exit(1)
    else:
        print("ğŸ‰ All documents valid")

if __name__ == "__main__":
    main()
```

### 5. æ¨ªæ–­æ¤œç´¢ãƒ»åˆ†æAPI

#### 5.1 ã‚¯ã‚¨ãƒªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
```python
class DocumentSearchAPI:
    def __init__(self, db_path: str):
        self.db = sqlite3.connect(db_path)
        
    def search_by_tags(self, tags: List[str]) -> List[Dict]:
        """ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹æ¤œç´¢"""
        placeholders = ','.join(['?' for _ in tags])
        query = f"""
            SELECT DISTINCT d.* FROM docs_index d
            JOIN doc_tags t ON d.id = t.doc_id
            WHERE t.tag IN ({placeholders})
        """
        return [dict(row) for row in self.db.execute(query, tags)]
        
    def search_by_category(self, category: str, status: str = 'active') -> List[Dict]:
        """ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ¤œç´¢"""
        query = """
            SELECT * FROM docs_index 
            WHERE category = ? AND status = ?
            ORDER BY updated DESC
        """
        return [dict(row) for row in self.db.execute(query, (category, status))]
        
    def get_complexity_metrics(self) -> Dict:
        """è¤‡é›‘æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        metrics = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        stats = self.db.execute("""
            SELECT 
                COUNT(*) as total_docs,
                AVG(lines) as avg_lines,
                MAX(lines) as max_lines,
                AVG(internal_links) as avg_links
            FROM docs_index WHERE status = 'active'
        """).fetchone()
        
        metrics['basic_stats'] = dict(stats) if stats else {}
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒ
        category_dist = self.db.execute("""
            SELECT category, COUNT(*) as count, AVG(lines) as avg_lines
            FROM docs_index WHERE status = 'active'
            GROUP BY category
        """).fetchall()
        
        metrics['category_distribution'] = [dict(row) for row in category_dist]
        
        # è¤‡é›‘æ€§ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆ
        hotspots = self.db.execute("""
            SELECT file_path, title, lines, internal_links
            FROM docs_index 
            WHERE lines > 500 OR internal_links > 10
            ORDER BY lines DESC, internal_links DESC
        """).fetchall()
        
        metrics['complexity_hotspots'] = [dict(row) for row in hotspots]
        
        return metrics
        
    def get_outdated_documents(self, days: int = 90) -> List[Dict]:
        """å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œå‡º"""
        query = """
            SELECT * FROM docs_index 
            WHERE status = 'active' 
              AND date(updated) < date('now', '-{} days')
            ORDER BY updated ASC
        """.format(days)
        
        return [dict(row) for row in self.db.execute(query)]
```

---

## ğŸ”§ å®Ÿè£…ä¾‹ãƒ»ä½¿ç”¨ä¾‹

### æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
```markdown
---
title: "æ–°æ©Ÿèƒ½è¨­è¨ˆæ›¸: ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼ã‚·ã‚¹ãƒ†ãƒ "
status: "draft"
category: "reference"
created: "2025-07-14"
updated: "2025-07-14"
tags: ["authentication", "security", "api"]
priority: "high"
author: "development-team"
review_date: "2025-07-21"
depends_on: ["003-security-requirements.md"]
---

# æ–°æ©Ÿèƒ½è¨­è¨ˆæ›¸: ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼ã‚·ã‚¹ãƒ†ãƒ 

## æ¦‚è¦
...
```

### è¤‡é›‘æ€§åˆ†æã‚¯ã‚¨ãƒªä¾‹
```sql
-- æœ€ã‚‚è¤‡é›‘ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ TOP 10
SELECT file_path, title, lines, internal_links,
       (lines * 0.1 + internal_links * 2) as complexity_score
FROM docs_index 
WHERE status = 'active'
ORDER BY complexity_score DESC
LIMIT 10;

-- ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡è¤‡é›‘æ€§
SELECT category, 
       COUNT(*) as docs,
       AVG(lines) as avg_lines,
       AVG(internal_links) as avg_links,
       AVG(lines * 0.1 + internal_links * 2) as avg_complexity
FROM docs_index 
WHERE status = 'active'
GROUP BY category;

-- æ›´æ–°é »åº¦åˆ†æ
SELECT 
    CASE 
        WHEN date(updated) > date('now', '-7 days') THEN 'Recent'
        WHEN date(updated) > date('now', '-30 days') THEN 'Month'
        WHEN date(updated) > date('now', '-90 days') THEN 'Quarter'
        ELSE 'Stale'
    END as freshness,
    COUNT(*) as count
FROM docs_index 
WHERE status = 'active'
GROUP BY freshness;
```

### CIçµ±åˆã§ã®ã‚¢ãƒ©ãƒ¼ãƒˆä¾‹
```bash
# è¤‡é›‘æ€§é•åæ¤œå‡º
echo "ğŸš¨ Document Complexity Violations:"
sqlite3 docs_index.db "
    SELECT file_path, lines FROM docs_index 
    WHERE lines > 500 AND status = 'active'
" | while read file lines; do
    echo "  - $file: $lines lines (> 500 limit)"
done

# æœªå®Œäº† Front-Matter æ¤œå‡º
echo "âš ï¸ Missing Front-Matter:"
find . -name "*.md" -exec grep -L "^---" {} \; | head -10
```

---

## ğŸ“ˆ åŠ¹æœæ¸¬å®šãƒ»KPIå®šç¾©

### å®Ÿè£…åŠ¹æœæŒ‡æ¨™
```sql
-- Front-Matteré©ç”¨ç‡
SELECT 
    (COUNT(CASE WHEN title IS NOT NULL THEN 1 END) * 100.0 / COUNT(*)) as coverage_rate
FROM docs_index;

-- è¤‡é›‘æ€§æ”¹å–„ç‡
SELECT 
    AVG(CASE WHEN lines <= 500 THEN 1.0 ELSE 0.0 END) as manageable_size_rate,
    AVG(CASE WHEN internal_links <= 10 THEN 1.0 ELSE 0.0 END) as manageable_links_rate
FROM docs_index WHERE status = 'active';

-- æ¤œç´¢åŠ¹ç‡æ”¹å–„
-- Before: æ‰‹å‹•grepæ¤œç´¢æ™‚é–“ vs After: SQLiteã‚¯ã‚¨ãƒªæ™‚é–“
```

### å®šæœŸãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
```python
def generate_weekly_report():
    """é€±æ¬¡è¤‡é›‘æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    api = DocumentSearchAPI('docs_index.db')
    
    report = {
        'timestamp': datetime.now(),
        'metrics': api.get_complexity_metrics(),
        'hotspots': api.get_complexity_hotspots(),
        'outdated': api.get_outdated_documents(30),
        'frontmatter_coverage': api.get_frontmatter_coverage()
    }
    
    # JSON/HTML ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    with open('build/weekly_complexity_report.json', 'w') as f:
        json.dump(report, f, indent=2, default=str)
        
    return report
```

---

## ğŸ”— ã‚·ã‚¹ãƒ†ãƒ çµ±åˆä»•æ§˜

### AIå®Œç’§ä¸»ç¾©é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆADR-003ï¼‰ã¨ã®çµ±åˆ
```bash
# constitution-guard.sh ã§ã®è¤‡é›‘æ€§ãƒã‚§ãƒƒã‚¯çµ±åˆ
check_document_complexity() {
    local files=("$@")
    
    for file in "${files[@]}"; do
        # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
        local lines=$(wc -l < "$file")
        if [[ $lines -gt 500 ]]; then
            echo "âš ï¸ è¤‡é›‘æ€§é•å: $file ($lines è¡Œ > 500è¡Œåˆ¶é™)"
            echo "ADR-005ã«å¾“ã„ã€ä»¥ä¸‹ã‚’æ¤œè¨ã—ã¦ãã ã•ã„:"
            echo "1. æ–‡æ›¸åˆ†å‰²"
            echo "2. æ§‹é€ åŒ–æ”¹å–„"  
            echo "3. ä¸è¦ãªè©³ç´°å‰Šé™¤"
            return 1
        fi
        
        # Front-Matterå¿…é ˆãƒã‚§ãƒƒã‚¯
        if ! head -1 "$file" | grep -q "^---"; then
            echo "âŒ Front-Matteræœªè¨­å®š: $file"
            echo "schemas/md-meta.schema.json ã«å¾“ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"
            return 1
        fi
    done
    
    return 0
}
```

### ç¶™ç¶šæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ï¼ˆADR-002ï¼‰ã¨ã®çµ±åˆ
```python
# improvement_recommendations.md ã¸ã®è‡ªå‹•çµ±åˆ
def update_improvement_recommendations():
    api = DocumentSearchAPI('docs_index.db')
    hotspots = api.get_complexity_hotspots()
    
    recommendations = []
    for doc in hotspots:
        if doc['lines'] > 500:
            recommendations.append({
                'id': f"DOC-COMPLEX-{doc['id']}",
                'category': 'Document Complexity',
                'description': f"å¤§å‹æ–‡æ›¸ã®åˆ†å‰²: {doc['file_path']} ({doc['lines']}è¡Œ)",
                'priority': 'medium',
                'implementation': 'ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ»åˆ¥æ–‡æ›¸åŒ–',
                'related_adr': 'ADR-005'
            })
            
    return recommendations
```

---

*ã“ã®æŠ€è¡“ä»•æ§˜æ›¸ã¯ã€ADR-005ã§æ±ºå®šã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¤‡é›‘æ€§åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãƒ»é‹ç”¨ãƒ»çµ±åˆã«å¿…è¦ãªå…¨æŠ€è¡“è©³ç´°ã‚’ç¶²ç¾…ã—ã€æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®ä¸€å…ƒçš„ãªå“è³ªç®¡ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚*

*ä½œæˆæ—¥: 2025-07-14*  
*ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.0*  
*æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼: Week 3 CIçµ±åˆå®Œäº†æ™‚*